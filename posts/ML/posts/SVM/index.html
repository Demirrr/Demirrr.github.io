
<!DOCTYPE html>
<html lang="en-us">
  <head>
      <title>wX</title>
      <style>

pre, code { color: #bdae9d; background-color: #2a211c; }
code > span.kw { color: #43a8ed; font-weight: bold; }
      </style>
  </head>
<body>

    <header>
      <h1 class="p-name entry-title" itemprop="headline name">
        Understanding Support Vector Machine 
      </h1>
</header>
    <div class="e-content entry-content" itemprop="articleBody text">

<p>The goal of this tutorial is to implement SVM from scratch while explaining it.</p> 

<p>I highly recommend the reader to firstly watch lectures which are given by 
<a href="https://www.coursera.org/learn/machine-learning/lecture/sKQoJ/using-an-svm">Andrew Ng</a>
and 
<a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/lecture-videos/lecture-16-learning-support-vector-machines/
">Patrick Winston</a>. 
While Andrew Ng provides very good intution, Patrick Winston shows the beauty of underlying matematics on SVM.</p>


<strong id="table-of-contents">Table of Contents</strong><nav id="TableOfContents">
<ul>
<li><a href="#Chapter1">Introduction to SVM</a></li>
<li><a href="#Chapter3">Visualize SVM</a></li>
<li><a href="#Chapter4">Implementation</a></li>
</nav>

<h1 id="Chapter1">Intro to SVM</h1>
	<p>
		SVM is one of the most popular approach in Machine Learning. Almost every AI book contains a chapter for SVM. There are many reasons why SVM is so important.  
		Following properties of SVM are important ones.

		<ul>
			<li> 
			SVM construct a maximum margin classifier, which finds a decision boundary with the largest distance to training instances. A classifier with the largest distance to training instances generalize well.
			According to <b>VC Analysis</b>, large margins imply smaller VC Dimension, smaller Growth function. Since SVM construct a maximum margin classifiert, it generalizes well, <a href="https://youtu.be/eHsErlPJWUU?t=8m7s">short expalanation by Professor Yaser Abu-Mostafa</a>. 
			</li>
			<p></p>
      
      		<li> For finding model parameter, <b>Kernel Trick</b> can be used. Kernel Trick is nothing but simply mapping the data into a higher-dimensional space without knowing the mapping.  SVM finds a decision boundary which maximizes the margin on any dataset, provided that data is linearly separable. Thank to Kernel Trick, dataset which is not linearly separable in the original input space are easily separable in the higher-dimensional space. The high-dimensional linear separator is actually nonlinear in the original space while being linear in induced/high-dimensional space. In fact, Vapnik had to wait 30 years to use idea of Kernel function in SVM. The beautiful story underlies in SVM and Kernel machines is explained by great lecturer Patrick Winston, <a href="https://youtu.be/_PwhiWxHK8o?t=46m15s">source</a>.
     		</li>
     		<p></p>
      
     		<li> Finding a model parameter is CONVEX optimization problem as apposed to many other algorithms, such as Neural Networks.
     		</li>

       </ul> 


	<p>		Consider the two plots in below. In the left image, there are five different decision boundary which perfectly classify all training instances. From the perceptive of loss function, there is nothing to worry, right?! However intutively, some examples are very close to the decision boundary. If one ponders on datagenerating process, this closeness is worring.
	</p>
	<p>
		SVM adress this issue of closeness to decision boundary; find a parametrization/decision boundary which correctly classfies all training points while keeping the margin as large as possible. More specifically, 

		Instead of minimizing expected empirical loss on the training data, SVM attempts to minimize expected generalization loss. We donâ€™t know where the
		as-yet-unseen points may fall, but under the probabilistic assumption that they are drawn from the same distribution as the previously seen examples, there are some arguments from computational learning theory suggesting that we minimize generalization loss by choosing the separator that is farthest away from the examples we have seen so far.
	</p>


	<img src="images/separating-lines.png" alt="Separating_lines"> 
	<img src="images/optimal-hyperplane.png" alt="Optimal Hyperplane">

<h2 id="Chapter3">Visualize SVM</h2>


<p><img src="images/Data_point.png" alt="Data Points"> <img src="images/LinearSVM.png" alt="LinearSVM.png"></p>


<p><img src="images/UnLinear_Datapoints.png" alt="UnLinear Data Points"> <img src="images/Poly_SVM.png" alt="Poly_SVM.png"></p>



<h2 id="Chapter4">Implementation</h2>
<a href="https://github.com/Demirrr/ML-Projects/blob/master/SVM/SVM%20from%20scratch.ipynb">SVM</a> 
python Implementation.
  </body>
</html>

