
<!DOCTYPE html>
<html lang="en-us">
  <head>
      <title>wX</title>
  </head>

	<body>

    	<header>
      		<h1 class="p-name entry-title" itemprop="headline name">
        		Logistic Regression
      		</h1>
		</header>
    
    <div class="e-content entry-content" itemprop="articleBody text">

	<p>	The logistic regression model is one of the most widely used binary models in the analysis of categorical data. It is a tool for representing the relation between a binary response and several predictors. Brilliant statistician David Cox has developed the logistic regression model in 1958. 	
	</p> 

	<strong id="table-of-contents">Table of Contents</strong>
	<nav id="TableOfContents">
		<ul>
		<li><a href="#Chapter0">Basic Understanding</a></li>

		<li><a href="#Chapter1">From Scratch Logistic Regression on Linearly Seperable Data</a>
			<ul>
				<li><a href="#Chapter1_2">Logistic Function</a></li>
				<li><a href="#Chapter1_3">How to find model parameters</a></li>
			</ul>
</li>
		<li><a href="#Chapter2">From Scratch Regularized Logistic Regression on Linearly UnSeperable Data</a>
		<ul>
			<li><a href="#Chapter2_1">Feature Mapping</a></li>
			<li><a href="#Chapter2_2">Be aware of Overfitting</a></li>
		</ul>
		</li>
	
		<li><a href="#Conclusion">Conclusion</a></li>
	</nav>


	<h1 id="Chapter0">Basic Understanding</h1>

	<p>	 In logistic regression, the probability of the positive class (and obviously of the negative class) is modelled in the form of <a href="https://en.wikipedia.org/wiki/Sigmoid_function">"sigmoid function"</a>, namely, <a href="https://en.wikipedia.org/wiki/Sigmoid_function">"logistic function"</a>. In other words, an instance belongs to one of the class with a certain probability P and hence belongs to the other class with "1 - P" probability.

		<p><img src="images/formula.png" alt="LogisticModel"></p>

		Note that, the larger the absoluve value 
<a href="http://www.codecogs.com/eqnedit.php?latex=\left&space;\|&space;w_i&space;\right&space;\|" target="_blank"><img src="http://latex.codecogs.com/gif.latex?\left&space;\|&space;w_i&space;\right&space;\|" title="\left \| w_i \right \|" /></a> of the regression coefficent, the stronger the influnce of the corresponding feature of instance. 



			<p><img src="images/model.png" alt="LogisticModel"></p>

	<p>	One should note that, <a href="http://www.codecogs.com/eqnedit.php?latex=h(x)&space;=&space;w^T&space;x" target="_blank"><img src="http://latex.codecogs.com/gif.latex?h(x)&space;=&space;w^T&space;x" title="h(x) = w^T x" /></a> the larger the response/score is the higher the probability of saying that given instance belongs the positive class.

	<p>It is also equivalent to modell the logatitm of the probability ration ( the log-odds ration) as a linear function of x.</p>
				<p><img src="images/log_ratio.png" alt="LogRatio"></p>


	<h1 id="Chapter1">From Scratch Logistic Regression on Linearly Seperable Data</h1>


	<p>
		The logistic regression hypothesis defined as

<a href="http://www.codecogs.com/eqnedit.php?latex=H(x)=g(W^{T}x)" target="_blank"><img src="http://latex.codecogs.com/gif.latex?H(x)=g(W^{T}x)" title="H(x)=g(W^{T}x)" /></a>

		where function <a href="http://www.codecogs.com/eqnedit.php?latex=g()" target="_blank"><img src="http://latex.codecogs.com/gif.latex?g()" title="g()" /></a>

		 is the sigmoid function. After sigmoid function is implemented. Loss function is impelmented. Loss function returns the loss and gradient.
	</p>

	<p><img src="images/before.png" alt="VisualizeData"></p>

	<h1 id="Chapter1_2">Logistic Function</h1>

					<p><img src="images/logistic_func.jpg" alt="logitic_f">.</p>


	
	<h1 id="Chapter1_3">How to find model parameters</h1>
	<a href="http://www.ece.northwestern.edu/local-apps/matlabhelp/toolbox/optim/fminunc.html">"fminunc</a> finds a minimum of a scalar function of several variables, starting at an initial estimate. This is generally referred to as unconstrained nonlinear optimization. The term unconstrained means that no restirction is placed on the rage of x. 

	<p> Fminunc is used to learn weights -W-.</p>

	<p> After learning the parameters, the model can be used to predict whether a particular student will be admitted.</p>


	<p><img src="images/after.png" alt="Model_fitted"></p>


	<h1 id="Chapter2">Regularized Logistic Regression</h1>

	<p>Regularized Logistic Regression is applied to the linearly unseperable data.</p>
	<p><img src="images/before2.png" alt="VisualizeData"></p>

	<p>
		As Plot show that dataset cannot be seperated into positive and negative examples by a straight-line. Therefore, without 'modification' Logistic Regression will not perform wel on this dataset since Logistic Regression will only be able to find a linear decision boundary.
	</p>

	<br>

	<h2 id="Chapter2_1">Feature Mapping</h2>
	Feature mapping enables to expand the the dimensions of feature space,namely, creates more features, so that possibly better chance to fit model. The features are mapped into all polynomial terms of x_1 and x_2 up to the sixth power. In other words, we are able to describe an object not from 2 perspective but right now 28 different perspective.
	<p>
	The vector of two features has been transfromed into a 28-dimensional vector. A logistic regression classifier trained on this higher-dimension feature vector will have a more complex decision boundary and will appear nonlinear when draw in our 2-dimensional plot.
	</p>

	<p>
		Regularization parameter is chosen to be 1,Lambda=1
	</p>

	<p><img src="images/noOverfitting.png" alt="VisualizeData"></p>
	
	

	<h2 id="Chapter2_2">Be aware of Overfitting</h2>

	<p>
		Lambda=0
	</p>
	<p><img src="images/Overfitting.png" alt="VisualizeData"></p>




	<h2 id="Conclusion">Conclusion</h2>

  </body>
</html>

